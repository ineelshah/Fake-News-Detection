{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "92acc_Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZSTT_eI3nVu",
        "colab_type": "code",
        "outputId": "6ab6d8f5-72f2-4280-90fd-12d55d6bc8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "#importing libraries\n",
        "#importing libraries-----------------------\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import Flatten, BatchNormalization, Concatenate, add\n",
        "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, Conv2D\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import Flatten, BatchNormalization, Concatenate, add\n",
        "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, Conv2D\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "MAX_LEN = 2000 \n",
        "VOCAB_SIZE = 0\n",
        "TEST_SIZE = 0.3\n",
        "\n",
        "def text_implict_preprocessing(texts):\n",
        "    # initialize tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    padded_texts = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "    return tokenizer, padded_texts, vocab_size\n",
        "\n",
        "def load_embedding_matrix(tokenizer, VOCAB_SIZE):\n",
        "    embeddings_index = dict()\n",
        "    f = open('drive/My Drive/glove_data/glove.6B/glove.6B.100d.txt')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    # create a weight matrix for words in training docs\n",
        "    embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Load input dataframe\n",
        "input_df = pd.read_csv('drive/My Drive/The_Research/all_data_refined_v5.csv', encoding='utf-8')\n",
        "\n",
        "# converting dataframe to numpy array\n",
        "input_data = input_df.to_numpy()\n",
        "\n",
        "# extract dataset and label\n",
        "X = np.delete(input_data, 6, 1)\n",
        "Y = input_data[:, 6]\n",
        "\n",
        "# Encoding the label\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_valid, Y_train, Y_valid=train_test_split(X, Y, test_size=TEST_SIZE, random_state=15)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "\n",
        "# Split dataset into branches\n",
        "\n",
        "# Text explicit branch\n",
        "X_train_text_explicit = np.concatenate((X_train[:,7:15], X_train[:,16:24], X_train[:,29:33]),axis=1)\n",
        "X_valid_text_explicit = np.concatenate((X_valid[:,7:15], X_valid[:,16:24], X_valid[:,29:33]),axis=1)\n",
        "\n",
        "# Text implicit branch\n",
        "tokenizer, padded_texts, VOCAB_SIZE = text_implict_preprocessing(np.concatenate((X_train[:,4],X_valid[:,4])))\n",
        "X_train_text_implicit = padded_texts[0:X_train.shape[0]]\n",
        "X_valid_text_implicit = padded_texts[X_train.shape[0]:]\n",
        "\n",
        "print(X_train_text_implicit.shape)\n",
        "print(X_valid_text_implicit.shape)\n",
        "\n",
        "# #Load Embedding Matrix\n",
        "embedding_matrix = load_embedding_matrix(tokenizer, VOCAB_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "# define Implicit\n",
        "inputs1 = Input(shape=(MAX_LEN,))\n",
        "e = Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False)(inputs1)\n",
        "a = (Dropout(0.5))(e)\n",
        "b = (Conv1D(filters=10, kernel_size=(4)))(a)\n",
        "c=MaxPooling1D(pool_size=2)(b)\n",
        "d=Flatten()(c)\n",
        "f=Dense(128)(d)\n",
        "g=(BatchNormalization())(f)\n",
        "z = Activation('relu')\n",
        "h=(Dropout(0.8))(g)\n",
        "\n",
        "# define Explicit\n",
        "inputs2 = Input(shape=(20,))\n",
        "q=(Dense(128))(inputs2)\n",
        "r=(BatchNormalization())(q)\n",
        "u=Activation('relu')(r)\n",
        "\n",
        "print(\"h: \", h)\n",
        "print(\"u: \", u)\n",
        "\n",
        "merged = concatenate([h, u])\n",
        "dense1 = Dense(10, activation='relu')(merged)\n",
        "outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(\"____________________\")\n",
        "print(model.summary())\n",
        "print(\"____________________\")\n",
        "\n",
        "plot_model(model, show_shapes=True, to_file='drive/My Drive/models/textCompiledModelRun3.png')\n",
        "\n",
        "# fit the model\n",
        "print(\"Fitting\")\n",
        "history = model.fit([X_train_text_implicit, X_train_text_explicit], array(Y_train), epochs=10, verbose=1, batch_size=16, validation_data=([X_valid_text_implicit, X_valid_text_explicit] , array(Y_valid)))\n",
        "print(\"Fitted\")\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"drive/My Drive/models/textCompiledModel.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/My Drive/models/textCompiledModelRun3.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "print(\"____________________\")\n",
        "loss, accuracy = model.evaluate([X_valid_text_implicit, X_valid_text_explicit] , array(Y_valid), verbose=1, batch_size=16)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "\n",
        "print(\"____________________\")\n",
        "output = model.predict([X_valid_text_implicit, X_valid_text_explicit])\n",
        "print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def finalOutputWithDelta(delta = 0.0):\n",
        "  for i in range(len(output)):\n",
        "    if output[i] >= (0.5 + delta):\n",
        "      output[i] = 1\n",
        "    elif output[i] < (0.5 + delta):\n",
        "      output[i] = 0\n",
        "\n",
        "  cm=confusion_matrix(Y_valid,output)\n",
        "  print(cm)\n",
        "\n",
        "  error = 0\n",
        "  correct = 0\n",
        "  for i in range(len(output)):\n",
        "    error = error + ((output[i] - Y_valid[i]) ** 2)\n",
        "\n",
        "    if output[i] == Y_valid[i]: correct+=1\n",
        "\n",
        "  print(\"error: \", error)\n",
        "  print(output)\n",
        "  print(correct/6605)\n",
        "\n",
        "finalOutputWithDelta()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig('drive/My Drive/models/textCompiledModelAccuracyRun2.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig('drive/My Drive/models/textCompiledModelLossRun2.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4515a31b04e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Load input dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0minput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/The_Research/all_data_refined_v5.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# converting dataframe to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/The_Research/all_data_refined_v5.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xpocFDZ_ztd",
        "colab_type": "code",
        "outputId": "0e46e150-8bd2-4ca4-e6e7-ebe5cda9cabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "'''\n",
        "# Loading the model:\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "import os\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('drive/My Drive/models/textCompiledModel.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"drive/My Drive/models/textCompiledModelRun2.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics= ['accuracy', 'mse'])\n",
        "score = loaded_model.evaluate([X_valid_text_implicit, X_valid_text_explicit] , array(Y_valid), verbose=1)\n",
        "\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
        "\n",
        "precision_score(Y_valid, output, average=None)\n",
        "tn, fp, fn, tp = confusion_matrix(Y_valid, output).ravel()\n",
        "\n",
        "prec = tp/(tp+fp)\n",
        "rec = tp/(tp+fn)\n",
        "\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"F1: \", 2*((prec*rec)/(prec+rec)))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "6005/6005 [==============================] - 16s 3ms/step\n",
            "accuracy: 89.93%\n",
            "Precision:  0.9383070301291249\n",
            "Recall:  0.8047579983593109\n",
            "F1:  0.8664164274674322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asQDr_ZWD0w4",
        "colab_type": "code",
        "outputId": "a2991b91-8af8-4104-8d58-9a6119890f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSolYlR_yWxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}